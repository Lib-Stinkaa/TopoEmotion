We sincerely thank all reviewers for their constructive feedback. We have addressed the concerns and made our complete implementation publicly available at https://github.com/Lib-Stinkaa/TopoEmotion for full reproducibility.

**Response to R6767 (Code & Reproducibility)**

We acknowledge the concern and have now released our complete codebase. While R6767 suggests this may be LLM-generated, we respectfully maintain this is original research. Our implementation includes the full pipeline from signal preprocessing to classification, with three comprehensive comparison studies demonstrating our approach's advantages.

First, we compare persistence landscape features against traditional time-frequency features and alternative topological representations (Betti curves). Our method achieves 76.53±8.39% accuracy on valence and 70.10±7.59% on arousal classification, representing a +17.0% improvement over conventional time-frequency features (59.53% valence, 57.53% arousal). This demonstrates that persistence landscapes capture emotion-relevant geometric structures invisible to traditional feature engineering—a fundamental contribution to affective computing.

Second, we validate our adaptive ensemble architecture against individual state-of-the-art classifiers and deep learning models. Our heterogeneous 10-classifier ensemble with F1-weighted voting significantly outperforms: RandomForest by +13.2% (63.32%→76.53%), CNN by +16.8% (59.75%→76.53%), MLP by +18.6% (58.97%→76.53%), and LSTM by +18.3% (57.00%→76.53%) on valence classification. The ensemble achieves F1 scores of 85.94±7.52% (valence) and 80.89±8.27% (arousal), addressing class imbalance naturally without synthetic resampling.

Third, we systematically evaluate multimodal fusion benefits. Eight-signal fusion achieves 76.53% accuracy compared to the best single modality (GSR at 69.56%), demonstrating +7.0% complementary information gain across physiological channels.

**Response to R38D4 (Implementation Details)**

All preprocessing, embedding optimization, feature extraction, baseline comparisons, and classification code are now available in the GitHub repository. The implementation enables direct replication of all reported results.

**Response to R7F07 (Parameter Selection)**

Embedding parameters in Table II are determined through established data-driven algorithms: delay τ from the first local minimum of Average Mutual Information (Fraser-Swinney method), and embedding dimension d from False Nearest Neighbors analysis with <10% threshold (Cao's algorithm). These are standard phase space reconstruction techniques from dynamical systems theory, not arbitrary choices. Landscape resolution (5 layers × 50 bins per homology dimension) was validated empirically to balance feature expressiveness and computational efficiency through cross-validation.

**Response to R72D6 (Validation Methodology)**

Our Leave-One-Subject-Out (LOSO) protocol ensures subject-independent evaluation—the gold standard for physiological computing generalization. Each of 30 subjects serves as the test set exactly once, with feature normalization computed only on training data within each fold to prevent data leakage. This is more rigorous than k-fold cross-validation, as it guarantees models generalize to completely unseen individuals. The reproducibility is ensured through fixed random seeds and complete code availability.

**Response to R0035 (Baseline Comparisons)**

We now provide three systematic comparison studies: (1) multimodal fusion demonstrating complementary information integration across 8 physiological signals versus individual modalities, (2) ensemble learning advantages over 6 traditional ML baselines (RandomForest, ExtraTrees, GradientBoosting, SVM, LogisticRegression, KNN) and 3 deep learning architectures (MLP, CNN, LSTM), and (3) topological features (persistence landscapes) outperforming both conventional time-frequency analysis and alternative topological representations (Betti curves). All experiments use natural class distribution without SMOTE or synthetic resampling, ensuring fair comparison under identical evaluation protocols.

**Response to R50DD (Methodological Rigor)**

We have strengthened our ablation studies as described above. The three-dimensional comparison framework isolates: (a) feature representation effects (topological vs. traditional), (b) classification strategy effects (ensemble vs. individual models), and (c) modality integration effects (fusion vs. single channels). All comparisons use LOSO validation with consistent hyperparameters and preprocessing, ensuring that performance differences reflect genuine methodological advantages rather than evaluation artifacts.

The complete codebase, detailed documentation, and all experimental results are publicly accessible at https://github.com/Lib-Stinkaa/TopoEmotion, enabling the community to verify, extend, and build upon this work.
